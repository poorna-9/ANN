import numpy as np

# Optimizer class per layer
class Optimizer:
    def __init__(self):
        self.m_w = 0
        self.v_w = 0
        self.m_b = 0
        self.v_b = 0
        self.t = 0

    def update(self, optimizer, w, dw, b, db, learning_rate, beta1=0.9, beta2=0.999, eps=1e-8):
        self.t += 1

        if optimizer == 'adam':
            self.m_w = beta1 * self.m_w + (1 - beta1) * dw
            self.m_b = beta1 * self.m_b + (1 - beta1) * db
            self.v_w = beta2 * self.v_w + (1 - beta2) * (dw ** 2)
            self.v_b = beta2 * self.v_b + (1 - beta2) * (db ** 2)

            m_w_hat = self.m_w / (1 - beta1 ** self.t)
            m_b_hat = self.m_b / (1 - beta1 ** self.t)
            v_w_hat = self.v_w / (1 - beta2 ** self.t)
            v_b_hat = self.v_b / (1 - beta2 ** self.t)

            w -= learning_rate * m_w_hat / (np.sqrt(v_w_hat) + eps)
            b -= learning_rate * m_b_hat / (np.sqrt(v_b_hat) + eps)

        elif optimizer == 'rmsprop':
            decay = beta1
            self.v_w = decay * self.v_w + (1 - decay) * (dw ** 2)
            self.v_b = decay * self.v_b + (1 - decay) * (db ** 2)
            w -= learning_rate * dw / (np.sqrt(self.v_w) + eps)
            b -= learning_rate * db / (np.sqrt(self.v_b) + eps)

        elif optimizer == 'momentum':
            self.v_w = beta1 * self.v_w + learning_rate * dw
            self.v_b = beta1 * self.v_b + learning_rate * db
            w -= self.v_w
            b -= self.v_b

        elif optimizer == 'sgd':
            w -= learning_rate * dw
            b -= learning_rate * db

        else:
            raise ValueError("Unsupported optimizer")

        return w, b

# ANN class
class ANN:
    def __init__(self):
        self.weightdict = {}
        self.biasdict = {}
        self.activations = []
        self.neurons = []
        self.activationvalues = []
        self.layercount = 0
        self.initmethod = 'he'
        self.loss = 'mse'
        self.optimizer = 'adam'
        self.opt = {} 

    def initializeweight(self, shape):
        fan_in, fan_out = shape
        if self.initmethod == 'he':
            limit = np.sqrt(2 / fan_in)
        elif self.initmethod == 'xavier':
            limit = np.sqrt(6 / (fan_in + fan_out))
        else:
            raise ValueError("Unsupported initialization method")
        return np.random.uniform(-limit, limit, size=shape)

    def add(self, neurons, activation, inputshape=None):
        if self.layercount == 0 and inputshape is not None:
            self.neurons.append(inputshape[1])
        self.neurons.append(neurons)
        self.activations.append(activation)
        self.weightdict[self.layercount] = self.initializeweight((self.neurons[-2], self.neurons[-1]))
        self.biasdict[self.layercount] = np.zeros((1, self.neurons[-1]))
        self.opt[self.layercount] = Optimizer()  
        self.layercount += 1

    def activation(self, x, act):
        if act == 'linear':
            return x
        elif act == 'sigmoid':
            x = np.clip(x, -500, 500)
            return 1 / (1 + np.exp(-x))
        elif act == 'tanh':
            return np.tanh(x)
        elif act == 'relu':
            return np.maximum(0, x)
        elif act == 'leakyrelu':
            return np.maximum(0.01 * x, x)
        elif act == 'softmax':
            exp_x = np.exp(x - np.max(x, axis=1, keepdims=True))
            return exp_x / np.sum(exp_x, axis=1, keepdims=True)
        else:
            raise ValueError("Unsupported activation")

    def activation_derivative(self, x, act):
        if act == 'linear':
            return np.ones_like(x)
        elif act == 'sigmoid':
            s = self.activation(x, 'sigmoid')
            return s * (1 - s)
        elif act == 'tanh':
            return 1 - np.tanh(x) ** 2
        elif act == 'relu':
            return (x > 0).astype(float)
        elif act == 'leakyrelu':
            return np.where(x > 0, 1, 0.01)
        elif act == 'softmax':
            # derivative handled in loss for cross-entropy
            return np.ones_like(x)
        else:
            raise ValueError("Unsupported activation")

    def forward(self, x):
        self.activationvalues = [x]
        for i in range(self.layercount):
            x = np.dot(x, self.weightdict[i]) + self.biasdict[i]
            x = self.activation(x, self.activations[i])
            self.activationvalues.append(x)
        return x

    def loss_derivative(self, y_true, y_pred):
        if self.loss == 'mse':
            return 2 * (y_pred - y_true) / y_true.shape[0]
        elif self.loss in ['binary_crossentropy', 'categorical_crossentropy']:
            # works with softmax output
            return (y_pred - y_true) / y_true.shape[0]
        else:
            raise ValueError("Unsupported loss function")

    def backward(self, y):
        delta = None
        for j in reversed(range(self.layercount)):
            a = self.activationvalues[j + 1]
            prev_a = self.activationvalues[j]

            # Last layer special case for cross-entropy
            if j == self.layercount - 1 and self.loss in ['binary_crossentropy', 'categorical_crossentropy']:
                delta = self.loss_derivative(y, a)
            else:
                if delta is None:
                    delta = self.loss_derivative(y, a)
                delta *= self.activation_derivative(a, self.activations[j])

            dw = np.dot(prev_a.T, delta)
            db = np.sum(delta, axis=0, keepdims=True)

            self.weightdict[j], self.biasdict[j] = self.opt[j].update(
                self.optimizer, self.weightdict[j], dw, self.biasdict[j], db, learning_rate=0.001
            )

            if j > 0:
                delta = np.dot(delta, self.weightdict[j].T)

    def fit(self, x, y, epochs=1000, learning_rate=0.001, batch_size=None):
        n_samples = x.shape[0]
        for epoch in range(epochs):
            if batch_size is None:
                self.forward(x)
                self.backward(y)
            else:
                indices = np.arange(n_samples)
                np.random.shuffle(indices)
                for start in range(0, n_samples, batch_size):
                    end = start + batch_size
                    batch_idx = indices[start:end]
                    batch_x = x[batch_idx]
                    batch_y = y[batch_idx]
                    self.forward(batch_x)
                    self.backward(batch_y)

    def predict(self, x):
        return self.forward(x)

    def accuracy(self, x, y_true):
        y_pred = self.predict(x)
        if y_pred.shape[1] > 1:
            pred_class = np.argmax(y_pred, axis=1)
            true_class = np.argmax(y_true, axis=1)
        else:
            pred_class = (y_pred > 0.5).astype(int)
            true_class = y_true
        return np.mean(pred_class == true_class)

