import numpy as np
import math

class ANN:
    def __init__(self):
        self.weightdict = {}
        self.biasdict = {}
        self.activations = []
        self.neurons = []
        self.activationvalues = []
        self.layercount = 0
        self.initmethod = 'he'
        self.loss = 'mse'

    def initializeweight(self, shape):
        if self.initmethod == 'he':
            limit = math.sqrt(6 / shape[0])
        elif self.initmethod == 'xavier':
            limit = math.sqrt(6 / sum(shape))
        else:
            raise ValueError("Unsupported initialization method. Choose 'xavier' or 'he'")
        return np.random.uniform(-limit, limit, size=shape)

    def add(self, neurons, activation, inputshape=None):
        if self.layercount == 0 and inputshape is not None:
            self.neurons.append(inputshape[1])
        self.neurons.append(neurons)
        self.activations.append(activation)

        self.weightdict[self.layercount] = self.initializeweight((self.neurons[-2], self.neurons[-1]))
        self.biasdict[self.layercount] = np.zeros((1, self.neurons[-1]))  # Initialize bias to zeros
        self.layercount += 1

    def activation(self, x, activation):
        if activation == 'linear':
            return x
        elif activation == 'sigmoid':
            return 1 / (1 + np.exp(-x))
        elif activation == 'tanh':
            return np.tanh(x)
        elif activation == 'relu':
            return np.maximum(0, x)
        elif activation == 'leakyrelu':
            return np.maximum(0.01 * x, x)
        elif activation == 'softmax':
            exp_x = np.exp(x - np.max(x, axis=1, keepdims=True))
            return exp_x / np.sum(exp_x, axis=1, keepdims=True)
        else:
            raise ValueError("Unsupported activation function.")

    def activationderivative(self, x, activation):
        if activation == 'linear':
            return np.ones_like(x)
        elif activation == 'sigmoid':
            sig = self.activation(x, 'sigmoid')
            return sig * (1 - sig)
        elif activation == 'tanh':
            return 1 - np.tanh(x) ** 2
        elif activation == 'relu':
            return np.where(x > 0, 1, 0)
        elif activation == 'leakyrelu':
            return np.where(x > 0, 1, 0.01)
        else:
            raise ValueError("Unsupported activation function.")

    def forward(self, x):
        self.activationvalues = [x]
        for i in range(self.layercount):
            x = np.dot(x, self.weightdict[i]) + self.biasdict[i]
            x = self.activation(x, self.activations[i])
            self.activationvalues.append(x)
        return x

    def lossderivative(self, y_true, y_pred):
        if self.loss == 'mse':
            return 2 * (y_pred - y_true)
        elif self.loss in ['binary_crossentropy', 'categorical_crossentropy']:
            return y_pred - y_true
        else:
            raise ValueError("Unsupported loss function.")

    def backward(self, x, y, z, learning_rate):
        delta = self.lossderivative(y, z)
        for j in range(self.layercount - 1, -1, -1):  
            delta *= self.activationderivative(self.activationvalues[j + 1], self.activations[j])
            self.weightdict[j] -= learning_rate * np.dot(self.activationvalues[j].T, delta)
            self.biasdict[j] -= learning_rate * np.sum(delta, axis=0, keepdims=True)
            delta = np.dot(delta, self.weightdict[j].T)  

    def fit(self, x, y, epochs, learning_rate):
        for _ in range(epochs):
            z = self.forward(x)
            self.backward(x, y, z, learning_rate)
        return self.weightdict, self.biasdict

    def predict(self, x):
        return self.forward(x)

